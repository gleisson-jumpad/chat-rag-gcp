import streamlit as st
import os
import uuid
from rag_utils import process_uploaded_file, is_supported_file, SUPPORTED_EXTENSIONS, get_existing_tables
import time

st.set_page_config(page_title="Chat RAG", layout="wide")

st.title("üí¨ Chat RAG ‚Äì Jumpad")

# Initialize session state for storing uploaded files info
if 'session_id' not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = []

# Sidebar menu
menu = st.sidebar.radio("Navega√ß√£o", [
    "üè† In√≠cio",
    "üîå Teste de Conex√£o com PostgreSQL",
    "üì• Upload e Vetoriza√ß√£o de Arquivos",
    "üîç Consulta com RAG",
    "üß™ Diagn√≥stico Avan√ßado",
])

# P√°ginas do menu
if menu == "üè† In√≠cio":
    st.subheader("Bem-vindo ao sistema de RAG da Jumpad!")
    st.markdown("Use o menu lateral para navegar entre testes e funcionalidades do sistema.")

elif menu == "üîå Teste de Conex√£o com PostgreSQL":
    st.subheader("üîå Teste de Conex√£o com o Banco")
    try:
        from db_config import get_pg_connection
        conn = get_pg_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT version();")
        version = cursor.fetchone()[0]
        st.success("‚úÖ Conex√£o estabelecida com sucesso!")
        st.code(version)
        cursor.close()
        conn.close()
    except Exception as e:
        st.error("‚ùå Erro ao conectar ao banco de dados:")
        st.code(str(e))

elif menu == "üì• Upload e Vetoriza√ß√£o de Arquivos":
    st.subheader("üì• Upload e Vetoriza√ß√£o de Arquivos")
    
    # Check if we need to import existing documents
    if len(st.session_state.uploaded_files) == 0:
        st.info("üìã Verificando documentos existentes no banco de dados...")
        try:
            # Get document tables using our utility function
            vector_tables = get_existing_tables()
            
            if vector_tables:
                # Connect to database to get document info
                from db_config import get_pg_connection
                conn = get_pg_connection()
                cursor = conn.cursor()
                
                doc_count = 0
                unique_files = set()
                
                for table_name in vector_tables:
                    try:
                        # Query to extract file names from metadata_
                        cursor.execute(f"""
                            SELECT DISTINCT metadata_->>'file_name' as file_name 
                            FROM {table_name}
                            WHERE metadata_->>'file_name' IS NOT NULL
                        """)
                        
                        # Get unique filenames
                        distinct_files = cursor.fetchall()
                        
                        if distinct_files:
                            for file_info in distinct_files:
                                file_name = file_info[0] if file_info[0] else f"Documento {doc_count+1}"
                                
                                # Only add if we haven't seen this file before
                                if file_name not in unique_files:
                                    unique_files.add(file_name)
                                    
                                    # Add document info to session state
                                    st.session_state.uploaded_files.append({
                                        'name': file_name,
                                        'size': 0,
                                        'table': table_name,
                                        'file_id': f"{table_name}_{doc_count}"
                                    })
                                    doc_count += 1
                    except Exception as e:
                        st.warning(f"Erro ao extrair informa√ß√µes da tabela {table_name}: {str(e)}")
                
                cursor.close()
                conn.close()
                
                st.success(f"‚úÖ Encontrados {len(st.session_state.uploaded_files)} documentos processados!")
            else:
                st.warning("Nenhuma tabela de vetores encontrada no banco de dados.")
        except Exception as e:
            st.warning(f"N√£o foi poss√≠vel verificar documentos existentes: {str(e)}")
    
    # Display supported file types
    st.info(f"Formatos suportados: {', '.join(SUPPORTED_EXTENSIONS)}")
    
    # File uploader
    uploaded_file = st.file_uploader("Escolha um arquivo para processar", type=[ext[1:] for ext in SUPPORTED_EXTENSIONS])
    
    if uploaded_file is not None:
        # Show file details
        st.write(f"**Arquivo:** {uploaded_file.name} ({uploaded_file.size} bytes)")
        
        # Process button
        if st.button("Processar e Vetorizar"):
            with st.spinner("Processando documento e gerando vetores..."):
                try:
                    # Use the utility function from rag_utils.py
                    index = process_uploaded_file(uploaded_file, st.session_state.session_id)
                    
                    # Store file info in session state
                    if uploaded_file.name not in [f['name'] for f in st.session_state.uploaded_files]:
                        st.session_state.uploaded_files.append({
                            'name': uploaded_file.name,
                            'size': uploaded_file.size,
                            'table': f"vectors_{st.session_state.session_id}"
                        })
                    
                    st.success(f"‚úÖ Documento '{uploaded_file.name}' processado e vetorizado com sucesso!")
                    st.info(f"Vetores armazenados na tabela: vectors_{st.session_state.session_id}")
                except Exception as e:
                    st.error(f"‚ùå Erro ao processar o documento: {str(e)}")
    
    # Display uploaded files
    if st.session_state.uploaded_files:
        st.subheader("Arquivos Processados")
        for idx, file_info in enumerate(st.session_state.uploaded_files):
            st.write(f"{idx+1}. **{file_info['name']}** - Tabela: {file_info['table']}")

elif menu == "üîç Consulta com RAG":
    st.subheader("üí¨ ChatRAG")
    
    # Initialize chat history if it doesn't exist
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # Initialize session state for uploaded files if needed
    if 'uploaded_files' not in st.session_state:
        st.session_state.uploaded_files = []
    
    # Default RAG setting
    use_rag = True
    selected_file_names = []
    selected_tables = []
    
    # Add OpenAI model selection in the sidebar
    with st.sidebar:
        st.subheader("Configura√ß√µes do Chat")
        
        # Model selection with cleaner layout
        st.markdown("**Modelo de IA:**")
        model_options = {
            "GPT-3.5 Turbo": "gpt-3.5-turbo",
            "GPT-4o": "gpt-4o",
            "GPT-4o-mini": "gpt-4o-mini"
        }
        selected_model = st.selectbox(
            "Selecione o modelo:",
            options=list(model_options.keys()),
            index=1,  # Default to GPT-4o
            label_visibility="collapsed"
        )
        model_id = model_options[selected_model]
        
        # RAG toggle with cleaner layout
        st.markdown("---")
        st.markdown("**Conhecimento dos Documentos:**")
        use_rag = st.toggle("Usar RAG", value=True)
    
    # Check for documents and load them if necessary (but don't show messages)
    if use_rag and len(st.session_state.uploaded_files) == 0:
        with st.spinner("Carregando documentos..."):
            try:
                from rag_utils import get_existing_tables
                vector_tables = get_existing_tables()
                
                if vector_tables:
                    # Connect to database to get document info
                    from db_config import get_pg_connection
                    conn = get_pg_connection()
                    cursor = conn.cursor()
                    
                    doc_count = 0
                    unique_files = set()
                    
                    for table_name in vector_tables:
                        try:
                            # Query to extract file names from metadata_
                            cursor.execute(f"""
                                SELECT DISTINCT metadata_->>'file_name' as file_name 
                                FROM {table_name}
                                WHERE metadata_->>'file_name' IS NOT NULL
                            """)
                            
                            # Get unique filenames
                            distinct_files = cursor.fetchall()
                            
                            if distinct_files:
                                for file_info in distinct_files:
                                    file_name = file_info[0] if file_info[0] else f"Documento {doc_count+1}"
                                    
                                    # Only add if we haven't seen this file before
                                    if file_name not in unique_files:
                                        unique_files.add(file_name)
                                        
                                        # Add document info to session state
                                        st.session_state.uploaded_files.append({
                                            'name': file_name,
                                            'size': 0,
                                            'table': table_name,
                                            'file_id': f"{table_name}_{doc_count}"
                                        })
                                        doc_count += 1
                        except Exception as e:
                            # Silently handle errors - this is a background task
                            pass
                    
                    cursor.close()
                    conn.close()
                    
                    st.rerun()
                else:
                    use_rag = False
            except Exception as e:
                use_rag = False
    
    # Update sidebar with document selection if documents are available
    with st.sidebar:
        # Show documents if available
        if st.session_state.uploaded_files and use_rag:
            # Get unique files
            unique_files = []
            for f in st.session_state.uploaded_files:
                if f["name"] not in [uf["name"] for uf in unique_files]:
                    unique_files.append(f)
            
            st.markdown(f"üìö **Documentos Dispon√≠veis:** {len(unique_files)}")
            
            # Multi-select for documents with cleaner styling
            file_options = [f["name"] for f in unique_files]
            
            # Use custom container for better styling
            selection_container = st.container(border=True)
            with selection_container:
                st.markdown("**Selecione os documentos:**")
                
                # Create checkboxes for each file
                selected_file_names = []
                selected_files = []
                for file_name in file_options:
                    if st.checkbox(file_name, value=True):
                        selected_file_names.append(file_name)
                        # Get all files with this name
                        selected_files.extend([f for f in st.session_state.uploaded_files if f["name"] == file_name])
                
                # Get tables from selected files        
                selected_tables = [f["table"] for f in selected_files]
            
            if not selected_file_names:
                st.warning("‚ö†Ô∏è Nenhum documento selecionado")
                use_rag = False
        elif use_rag:
            st.warning("‚ö†Ô∏è Nenhum documento dispon√≠vel")
            use_rag = False
        
        # Add reset button at the bottom of sidebar
        st.markdown("---")
        if st.button("üóëÔ∏è Limpar Conversa", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    # Main chat area - style with a light background
    chat_container = st.container(border=False)
    with chat_container:
        # Display chat messages in a scrollable area
        st.markdown("### Conversa")
        message_container = st.container(height=400, border=True)
        with message_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
        
        # Chat input at the bottom
        st.markdown("### Digite sua pergunta")
        prompt = st.chat_input("Converse com o sistema...")
        
        if prompt:
            # Add user message to chat history
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # Create a placeholder for "processing" status
            with st.status("Processando mensagem...", expanded=False) as status:
                status.update(label="Enviando pergunta...", state="running")
                # Display user message immediately 
                st.rerun()
    
    # Process the last message if it's from the user and hasn't been answered yet
    if (st.session_state.messages and 
        len(st.session_state.messages) % 2 == 1 and 
        st.session_state.messages[-1]["role"] == "user"):
        
        # Get the last message
        last_message = st.session_state.messages[-1]["content"]
        
        # Display assistant response in chat message container
        with chat_container:
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                # Outer try block to catch all errors
                try:
                    # Show a spinner while processing
                    with st.spinner("Gerando resposta..."):
                        # Initialize LLM
                        try:
                            from llama_index.llms.openai import OpenAI
                            llm = OpenAI(model=model_id)
                        except Exception as e:
                            raise Exception(f"Erro ao inicializar LLM: {str(e)}")

                        # Check for greeting patterns
                        simple_greetings = [
                            "oi", "ol√°", "ola", "bom dia", "boa tarde", "boa noite", 
                            "hello", "hi", "hey", "good morning", "good afternoon", 
                            "good evening", "tudo bem", "como vai"
                        ]
                        
                        is_simple_greeting = False
                        clean_message = last_message.lower().strip()
                        for greeting in simple_greetings:
                            if greeting in clean_message or clean_message == greeting:
                                is_simple_greeting = True
                                break
                        
                        # Check if it's a system query
                        is_system_query = False
                        system_terms = ["rag", "arquivos", "documentos", "sistema", "vetores", "vectores", "embeddings", "como funciona"]
                        for term in system_terms:
                            if term in clean_message:
                                is_system_query = True
                                break
                        
                        # CASE 1: Simple greeting or no RAG available
                        if is_simple_greeting or (not use_rag or not selected_file_names):
                            try:
                                from llama_index.core.llms import ChatMessage, MessageRole
                                
                                # Create chat history
                                chat_history = []
                                for msg in st.session_state.messages[:-1]:
                                    role = MessageRole.USER if msg["role"] == "user" else MessageRole.ASSISTANT
                                    chat_history.append(ChatMessage(role=role, content=msg["content"]))
                                
                                # Get response
                                response = llm.stream_chat(
                                    messages=[
                                        *chat_history,
                                        ChatMessage(role=MessageRole.USER, content=last_message)
                                    ]
                                )
                                
                                # Stream response
                                for chunk in response:
                                    if chunk.delta:
                                        full_response += chunk.delta
                                        message_placeholder.markdown(full_response + "‚ñå")
                                        time.sleep(0.01)
                            except Exception as e:
                                raise Exception(f"Erro no chat b√°sico: {str(e)}")
                        
                        # CASE 2: RAG with system query about RAG itself
                        elif use_rag and is_system_query:
                            try:
                                # Get file info
                                file_info = []
                                for file in selected_file_names:
                                    file_info.append(f"- {file}")
                                
                                files_list = "\n".join(file_info)
                                
                                system_info = f"""
                                Este √© um sistema RAG (Retrieval-Augmented Generation) que permite consultar documentos vetorizados.
                                
                                Documentos dispon√≠veis:
                                {files_list}
                                
                                O sistema utiliza LlamaIndex e embeddings da OpenAI para processar documentos e responder perguntas
                                com base no conte√∫do desses documentos. A tecnologia RAG permite que o modelo de linguagem (LLM)
                                tenha acesso ao conte√∫do dos documentos para fornecer respostas mais precisas e contextualizadas.
                                """
                                
                                # Create prompt
                                prompt = f"""
                                O usu√°rio perguntou: "{last_message}"
                                
                                Responda usando as seguintes informa√ß√µes sobre o sistema:
                                
                                {system_info}
                                
                                Sua resposta deve explicar de forma clara o que √© RAG, como funciona o sistema,
                                e quais documentos est√£o dispon√≠veis para consulta.
                                """
                                
                                # Get response
                                response = llm.complete(prompt)
                                
                                # Stream response
                                for char in response.text:
                                    full_response += char
                                    message_placeholder.markdown(full_response + "‚ñå")
                                    time.sleep(0.01)
                            except Exception as e:
                                raise Exception(f"Erro ao processar informa√ß√µes do sistema: {str(e)}")
                        
                        # CASE 3: RAG with regular query
                        elif use_rag:
                            try:
                                message_placeholder.markdown("üîç Consultando documentos... Por favor aguarde.")
                                
                                # Check for document summarization requests
                                is_summary_request = False
                                summary_terms = ["resuma", "resumir", "resumo", "explique", "explicar", "sobre"]
                                for term in summary_terms:
                                    if term in clean_message.lower():
                                        is_summary_request = True
                                        break
                                
                                # If summarization request, check for specific document mention
                                if is_summary_request:
                                    doc_name = None
                                    for file in selected_file_names:
                                        if file.lower() in clean_message.lower():
                                            doc_name = file
                                            break
                                    
                                    # If specific document is mentioned, attempt direct summarization
                                    if doc_name:
                                        try:
                                            message_placeholder.markdown(f"üìÑ Buscando conte√∫do do documento: {doc_name}...")
                                            
                                            # Connect to database
                                            from db_config import get_pg_connection
                                            conn = get_pg_connection()
                                            cursor = conn.cursor()
                                            
                                            # Find the right table for this document
                                            doc_table = None
                                            for idx, name in enumerate(selected_file_names):
                                                if name == doc_name and idx < len(selected_tables):
                                                    doc_table = selected_tables[idx]
                                                    break
                                            
                                            if not doc_table:
                                                # Fall back to searching all tables
                                                doc_table = selected_tables[0]
                                            
                                            # Query document content
                                            cursor.execute(f"""
                                                SELECT 
                                                    content, 
                                                    metadata_->>'page_label' as page
                                                FROM {doc_table}
                                                WHERE metadata_->>'file_name' ILIKE '%{doc_name.replace("'", "''")}%'
                                                ORDER BY 
                                                    CASE 
                                                        WHEN metadata_->>'page_label' ~ '^[0-9]+$' THEN CAST(metadata_->>'page_label' AS INTEGER)
                                                        ELSE 9999
                                                    END ASC
                                                LIMIT 20;
                                            """)
                                            
                                            # Collect document content by page
                                            doc_contents = []
                                            for result in cursor.fetchall():
                                                content = result[0] if result[0] else ""
                                                page = result[1] if result[1] else ""
                                                doc_contents.append(f"[P√°gina {page}]\n{content}")
                                            
                                            cursor.close()
                                            conn.close()
                                            
                                            # If we have content, create a summary
                                            if doc_contents:
                                                message_placeholder.empty()
                                                
                                                # Create document context
                                                doc_text = "\n\n".join(doc_contents)
                                                
                                                # Create prompt for summarization
                                                summary_prompt = f"""
                                                Por favor, crie um resumo do seguinte documento: {doc_name}
                                                
                                                Conte√∫do do documento:
                                                
                                                {doc_text}
                                                
                                                Instru√ß√µes:
                                                1. Crie um resumo abrangente do documento acima
                                                2. Destaque os pontos-chave e principais t√≥picos
                                                3. Mantenha o resumo claro e conciso, mas informativo
                                                4. Mencione que este resumo √© baseado no arquivo {doc_name}
                                                """
                                                
                                                # Get summary
                                                response = llm.complete(summary_prompt)
                                                
                                                # Stream response
                                                for char in response.text:
                                                    full_response += char
                                                    message_placeholder.markdown(full_response + "‚ñå")
                                                    time.sleep(0.01)
                                                
                                                # Update response and finish
                                                message_placeholder.markdown(full_response)
                                                st.session_state.messages.append({"role": "assistant", "content": full_response})
                                                st.rerun()
                                        except Exception as summary_error:
                                            # Log error and continue with normal RAG process
                                            message_placeholder.empty()
                                            message_placeholder.markdown(f"‚ö†Ô∏è Erro ao resumir documento: {str(summary_error)}. Tentando abordagem alternativa...")
                                
                                # Get document content from database (normal RAG process)
                                try:
                                    from db_config import get_pg_connection
                                    conn = get_pg_connection()
                                    cursor = conn.cursor()
                                    
                                    results = []
                                    for table_name in selected_tables:
                                        try:
                                            cursor.execute(f"""
                                                SELECT 
                                                    content, 
                                                    metadata_->>'file_name' as filename,
                                                    metadata_->>'page_label' as page
                                                FROM {table_name}
                                                WHERE 
                                                    (content ILIKE '%{last_message.replace("'", "''")}%' OR 
                                                     metadata_->>'file_name' ILIKE '%{last_message.replace("'", "''")}%')
                                                LIMIT 10;
                                            """)
                                            
                                            for result in cursor.fetchall():
                                                content = result[0] if result[0] else ""
                                                filename = result[1] if result[1] else "Unknown"
                                                page = result[2] if result[2] else ""
                                                page_info = f" (P√°gina {page})" if page else ""
                                                
                                                results.append({
                                                    "content": content, 
                                                    "filename": f"{filename}{page_info}"
                                                })
                                        except Exception as table_error:
                                            # Skip failed tables
                                            pass
                                    
                                    cursor.close()
                                    conn.close()
                                    
                                    # If we found content, use it
                                    if results:
                                        # Clear loading message
                                        message_placeholder.empty()
                                        
                                        # Create context
                                        context = "\n\n".join([f"De {r['filename']}:\n{r['content']}" for r in results])
                                        
                                        # Create prompt with context
                                        formatted_prompt = f"""
                                        Baseado no seguinte contexto:
                                        
                                        {context}
                                        
                                        Por favor, responda esta pergunta: {last_message}
                                        
                                        D√™ sua resposta baseada APENAS nas informa√ß√µes do contexto fornecido. 
                                        Se a resposta n√£o estiver no contexto, diga "N√£o tenho essa informa√ß√£o no contexto fornecido."
                                        
                                        Importante: Voc√™ √© um assistente para um sistema RAG (Retrieval-Augmented Generation) 
                                        e tem acesso aos documentos que foram vetorizados. Quando perguntarem sobre 
                                        "arquivos RAG" ou semelhantes, explique sobre os documentos dispon√≠veis no sistema.
                                        """
                                        
                                        # Get response with context
                                        chat_response = llm.complete(formatted_prompt)
                                        
                                        # Stream response
                                        for char in chat_response.text:
                                            full_response += char
                                            message_placeholder.markdown(full_response + "‚ñå")
                                            time.sleep(0.01)
                                    else:
                                        # Try advanced RAG with vector search
                                        message_placeholder.empty()
                                        message_placeholder.markdown("‚ö†Ô∏è Sem resultados diretos. Tentando RAG avan√ßado...")
                                        
                                        try:
                                            # Import vector tools
                                            from llama_index.vector_stores.postgres import PGVectorStore
                                            from llama_index.core import VectorStoreIndex, StorageContext
                                            from llama_index.embeddings.openai import OpenAIEmbedding
                                            from llama_index.core.indices.query.query_transform.base import StepDecomposeQueryTransform
                                            from llama_index.core.query_engine import RouterQueryEngine
                                            
                                            # Database params
                                            conn_params = {
                                                "host": "34.150.190.157",
                                                "port": 5432, 
                                                "dbname": "postgres",
                                                "user": "llamaindex",
                                                "password": "password123"
                                            }
                                            
                                            # Create embedding model
                                            embed_model = OpenAIEmbedding(model="text-embedding-ada-002")
                                            
                                            # Helper function to create query engine for one table
                                            def create_engine_for_table(table_name):
                                                vector_store = PGVectorStore.from_params(
                                                    host=conn_params["host"],
                                                    port=conn_params["port"],
                                                    database=conn_params["dbname"],
                                                    user=conn_params["user"],
                                                    password=conn_params["password"],
                                                    table_name=table_name,
                                                    embed_dim=1536
                                                )
                                                
                                                storage_context = StorageContext.from_defaults(vector_store=vector_store)
                                                index = VectorStoreIndex.from_vector_store(
                                                    vector_store,
                                                    embed_model=embed_model
                                                )
                                                
                                                engine = index.as_query_engine(
                                                    llm=llm,
                                                    similarity_top_k=3
                                                )
                                                
                                                # Add system prompt if possible
                                                system_prompt = """You are an AI assistant for a RAG system. 
                                                You have access to documents that have been processed and vectorized. 
                                                When users ask about RAG files, explain about the available documents.
                                                Always use retrieved context to answer document questions."""
                                                
                                                if hasattr(engine, 'set_prompts'):
                                                    engine.set_prompts(system_prompt=system_prompt)
                                                
                                                return engine
                                            
                                            # Create query engine based on document count
                                            if len(selected_tables) == 1:
                                                # Single document case
                                                query_engine = create_engine_for_table(selected_tables[0])
                                                query_engine.similarity_top_k = 3
                                                if hasattr(query_engine, 'streaming'):
                                                    query_engine.streaming = True
                                            else:
                                                # Multiple documents case
                                                query_engines = {}
                                                file_tables = list(zip(selected_file_names, selected_tables))
                                                
                                                for i, (file_name, table) in enumerate(file_tables):
                                                    engine = create_engine_for_table(table)
                                                    query_engines[f"{file_name}_{i}"] = engine
                                                
                                                step_transform = StepDecomposeQueryTransform(
                                                    llm=llm,
                                                    verbose=True
                                                )
                                                
                                                query_engine = RouterQueryEngine(
                                                    selector="llm",
                                                    query_engines=query_engines,
                                                    llm=llm,
                                                    query_transform=step_transform
                                                )
                                            
                                            # Execute query
                                            message_placeholder.empty()
                                            response = query_engine.query(last_message)
                                            
                                            # Handle streaming vs non-streaming
                                            if hasattr(response, 'response_gen'):
                                                for token in response.response_gen:
                                                    full_response += token
                                                    message_placeholder.markdown(full_response + "‚ñå")
                                                    time.sleep(0.01)
                                            else:
                                                full_response = str(response)
                                                message_placeholder.markdown(full_response)
                                        except Exception as vector_error:
                                            # Fall back to regular chat
                                            message_placeholder.empty()
                                            message_placeholder.markdown(f"‚ö†Ô∏è RAG avan√ßado falhou: {str(vector_error)}. Usando chat normal...")
                                            
                                            from llama_index.core.llms import ChatMessage, MessageRole
                                            chat_history = []
                                            for msg in st.session_state.messages[:-1]:
                                                role = MessageRole.USER if msg["role"] == "user" else MessageRole.ASSISTANT
                                                chat_history.append(ChatMessage(role=role, content=msg["content"]))
                                            
                                            response = llm.stream_chat(
                                                messages=[
                                                    *chat_history,
                                                    ChatMessage(role=MessageRole.USER, content=last_message)
                                                ]
                                            )
                                            
                                            full_response = ""
                                            for chunk in response:
                                                if chunk.delta:
                                                    full_response += chunk.delta
                                                    message_placeholder.markdown(full_response + "‚ñå")
                                                    time.sleep(0.01)
                                    except Exception as db_error:
                                        raise Exception(f"Erro ao acessar banco de dados: {str(db_error)}")
                            except Exception as rag_error:
                                raise Exception(f"Erro no processo RAG: {str(rag_error)}")
                    
                    # Display final response and add to history
                    message_placeholder.markdown(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                
                except Exception as e:
                    # Handle any errors
                    error_message = f"Erro: {str(e)}"
                    message_placeholder.markdown(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
                
                # Rerun to update UI
                st.rerun()

elif menu == "üß™ Diagn√≥stico Avan√ßado":
    st.subheader("üß™ Diagn√≥stico e Depura√ß√£o")
    
    # Database connection test
    st.subheader("Teste de Conex√£o ao Banco de Dados")
    
    if st.button("Testar Conex√£o ao PostgreSQL"):
        try:
            from db_config import get_pg_connection
            conn = get_pg_connection()
            
            # Test the connection by executing a simple query
            cursor = conn.cursor()
            cursor.execute("SELECT version();")
            version = cursor.fetchone()[0]
            
            st.success("‚úÖ Conex√£o estabelecida com sucesso!")
            st.code(version)
            
            cursor.close()
            conn.close()
        except Exception as e:
            st.error(f"‚ùå Erro ao conectar ao banco de dados: {str(e)}")
    
    # Check for vector tables
    st.subheader("Tabelas de Vetores no Banco")
    
    if st.button("Verificar Tabelas de Vetores"):
        try:
            from rag_utils import get_existing_tables
            vector_tables = get_existing_tables()
            
            if vector_tables:
                st.success(f"‚úÖ Encontradas {len(vector_tables)} tabelas de vetores:")
                for table in vector_tables:
                    st.code(table)
            else:
                st.warning("‚ö†Ô∏è N√£o foram encontradas tabelas de vetores no banco de dados.")
        except Exception as e:
            st.error(f"‚ùå Erro ao verificar tabelas: {str(e)}")
    
    # Environment variables check
    st.subheader("Vari√°veis de Ambiente")
    
    if st.button("Verificar Vari√°veis de Ambiente"):
        env_vars = {
            "DB_PUBLIC_IP": os.getenv("DB_PUBLIC_IP"),
            "PG_PORT": os.getenv("PG_PORT"),
            "PG_DB": os.getenv("PG_DB"),
            "PG_USER": os.getenv("PG_USER"),
            "PG_PASSWORD": "*****" if os.getenv("PG_PASSWORD") else None,
            "OPENAI_API_KEY": "*****" if os.getenv("OPENAI_API_KEY") else None
        }
        
        st.json(env_vars)
    
    # Session state debug
    st.subheader("Estado da Sess√£o (Session State)")
    
    if st.button("Verificar Estado da Sess√£o"):
        debug_info = {
            "session_id": st.session_state.get("session_id", None),
            "uploaded_files_count": len(st.session_state.get("uploaded_files", [])),
            "uploaded_files": st.session_state.get("uploaded_files", []),
            "messages_count": len(st.session_state.get("messages", []))
        }
        
        st.json(debug_info)
    
    # Force refresh session
    if st.button("Limpar e Recarregar Sess√£o"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.success("Sess√£o limpa com sucesso!")
        st.rerun()
    
    # Database query tool
    st.subheader("Ferramenta de Consulta SQL")
    
    # Predefined queries
    predefined_queries = {
        "Contar documentos distintos": "SELECT COUNT(DISTINCT metadata_->>'file_name') as documentos_distintos FROM data_vectors_5bed5a54_76f3_4a10_bb16_176d8fecc104;",
        "Listar arquivos distintos": "SELECT DISTINCT metadata_->>'file_name' as nome_arquivo FROM data_vectors_5bed5a54_76f3_4a10_bb16_176d8fecc104;",
        "Contar chunks por arquivo": "SELECT metadata_->>'file_name' as nome_arquivo, COUNT(*) as numero_chunks FROM data_vectors_5bed5a54_76f3_4a10_bb16_176d8fecc104 GROUP BY metadata_->>'file_name';",
        "Ver metadados dos documentos": "SELECT id, metadata_ FROM data_vectors_5bed5a54_76f3_4a10_bb16_176d8fecc104 LIMIT 10;",
        "Ver conte√∫do dos documentos": "SELECT id, metadata_->>'file_name' as nome_arquivo, SUBSTRING(content, 1, 200) as preview_conteudo FROM data_vectors_5bed5a54_76f3_4a10_bb16_176d8fecc104 LIMIT 5;",
        "Listar todas as tabelas": "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND (table_name LIKE 'vectors_%' OR table_name LIKE 'data_vectors_%');",
        "Ver estrutura da tabela": "SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'data_vectors_5bed5a54_76f3_4a10_bb16_176d8fecc104';",
    }
    
    # Query selector
    selected_query = st.selectbox(
        "Consultas pr√©-definidas:",
        options=list(predefined_queries.keys())
    )
    
    # Text area for SQL query
    sql_query = st.text_area(
        "Digite sua consulta SQL:",
        predefined_queries[selected_query],
        height=100
    )
    
    # Execute button
    if st.button("Executar Consulta"):
        try:
            from db_config import get_pg_connection
            conn = get_pg_connection()
            cursor = conn.cursor()
            
            # Execute the query
            cursor.execute(sql_query)
            
            # Get the results
            results = cursor.fetchall()
            column_names = [desc[0] for desc in cursor.description]
            
            # Display the results
            if results:
                st.success(f"‚úÖ Consulta executada com sucesso: {len(results)} registros encontrados")
                
                # Create a DataFrame for display
                import pandas as pd
                df = pd.DataFrame(results, columns=column_names)
                st.dataframe(df)
                
                # Also show as raw text (useful for copying values)
                st.code(str(results))
            else:
                st.info("A consulta foi executada, mas n√£o retornou resultados.")
            
            cursor.close()
            conn.close()
        except Exception as e:
            st.error(f"‚ùå Erro ao executar a consulta: {str(e)}")
